\section{Introduction}

Self-organizing map \citep{Kohonen:1982} is a vector quantization method that
maps data onto a regular grid, usually two-dimensional. After learning has
converged, the codebook is self-organized such that the prototypes associated
with two nearby nodes are similar. This is a direct consequence of the
underlying topology of the map as well as the learning algorithm that, when
presented with a new sample, modifies the prototype of the best matching unit
(BMU) as well as the units in its vicinity (neighborhood). SOM have been used
in a huge number of applications \citep{Kaski:1998,Oja:2003,Polla:2009} and
there exists today several variants of the original algorithm
\citep{Kohonen:2001}. However, according to the survey of
\citet{Astudillo:2014}, only a few of these variants consider an alternative
topology for the map, the regular Cartesian and the hexagonal grid being by far
the most common used ones. Among the alternatives, the growing neural gas
\citep{Fritzke:1994} is worth to be mentionned since it relies on a dynamic set
of units and builds the topology {\em a posteriori} as it is also the case for
the incremental grid growing neural network \citep{Blackmore:1995} and the
controled growth self organizing map \citep{Alahakoon:2000}. However, this {\em
  a posteriori} topology is built in the data space as opposed to the neural
space. This means that the neighborhood property is lost and two neurons that
are close to each other on the map may end with totally different prototypes in
the data space. The impact of the network topology on the self-organization has
also been studied by \citet{Jiang:2009} using the MNIST database. In the direct
problem, these authors consider SOMs whose neighborhood is defined by a
regular, small world or random network and show a weak influence of the
topology on the performance of the underlying model. In the inverse problem,
authors tries to optimize the topology of the network using evolutionnary
algorithms \citep{Eiben:2003} in order to minimize the classification error and
results indicate a weak correlation between the topology and the performances
in this specific case. This conclusion by \citet{Burguillo:2013} is a bit
different. Authors studied self-organizing map for time serie predictions and
considered various topologies (spatial, small-world, random and
scale-free). They concluded that the classical spatial topology remains the
best while the scale-free topology seems inadequate for the time series
prediction task. But for the two others, the difference was not so large and
topology doed not seem to dramatically impact performance.\\

In this work, we are interested in exploring alternative topology in order to
solve one of the fundamental problem related to SOM: the intrinsic dimension of
the data may or may not correspond to the dimension of the network. Most of
the time, the topology of the SOM is one dimensional (linear network) or two
dimensional (regular or hexagonal grid) and this may not correspond to the
intrinsic dimension of the data, especially in the high dimensional case. This
may result in the non-preservation of the topology \citep{Villmann:1999}
with potentially multiple foldings of the map. The problem is made even harder
considering the data are unknown at the time of construction of the network.
To overcome this topological constraint, we propose a variation of the self
organizing map algorithm by considering the random placement of neurons on a
two-dimensional manifold, following a blue noise distribution from which
various topologies can be derived. These topologies possess random (but
controllable) discontinuities that allow for a more flexible self-organization,
especially with high-dimensional data. After introducing the methods, the model
will be illustrated using several classical examples and its properties will be
more finely studied. Finally, we'll explain how this model can be made
resilient to neural gain or loss by reorganizing the neural sheet using
centroidal Voronoi tesselation.


% of the original topology in order ot fit the data, but the topology
% constraints may hinder such folding.
