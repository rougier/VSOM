\section{Introduction}

Self-organizing map \citep{Kohonen:1982} (SOM) is a vector
quantization method that maps data onto a grid, usually
two-dimensional and regular. After learning has converged, the
codebook is self-organized such that the prototypes associated with
two nearby nodes are similar. This is a direct consequence of the
underlying topology of the map as well as the learning algorithm that,
when presented with a new sample, modifies the code word of the best
matching unit (BMU, the unit with the closest to the input code word)
as well as the code word of units in its vicinity (neighborhood). SOMs
have been used in a vast number of applications
\citep{Kaski:1998,Oja:2003,Polla:2009} and today there exist several
variants of the original algorithm \citep{Kohonen:2001}. However,
according to the survey of \citep{Astudillo:2014}, only a few of these
variants consider an alternative topology for the map, the regular
Cartesian and the hexagonal grid being by far the most common used
ones. Among the alternatives, the growing neural gas
\citep{Fritzke:1994} is worth to be mentioned since it relies on a
dynamic set of units and builds the topology {\em a posteriori} as it
is also the case for the incremental grid growing neural network
\citep{Blackmore:1995} and the controlled growth self organizing map
\citep{Alahakoon:2000}. However, this {\em a posteriori} topology is
built in the data space as opposed to the neural space. This means
that the neighborhood property is lost and two neurons that are close
to each other on the map may end with totally different prototypes in
the data space. The impact of the network topology on the
self-organization has also been studied by \citep{Jiang:2009} using
the MNIST database. In the direct problem (evaluating influence of
topology on performance), these authors consider SOMs whose
neighborhood is defined by a regular, small world or random network
and show a weak influence of the topology on the performance of the
underlying model. In the inverse problem (searching for the best
topology), authors try to optimize the topology of the network using
evolutionary algorithms \citep{Eiben:2003} in order to minimize the
classification error. Their results indicate a weak correlation
between the topology and the performances in this specific
case. However, \citep{Burguillo:2013} reported contradictory results
to \citep{Eiben:2003}, when they studied the use of self-organizing
map for time series predictions and considered different topologies
(spatial, small-world, random and scale-free). They concluded that the
classical spatial topology remains the best while the scale-free
topology seems inadequate for the time series prediction task. But for
the two others (random and small-world), the difference was not so
large and topology does not seem to dramatically impact performance.
%
\correction{An alternative approach is to consider the set of neural
  nodes as an undirected planar graph in order to take advantage of a
  generalized adjacency measure (shortest graph distance)
  \citep{Barsi:2003}. This idea has been further explored in
  \citep{Come:2010,Come:2015} where authors take advantage of a star
  topology to map different clusters. However, this approach cannot
  be easily generalized because the topology is tailored to the input data. }
%
In this work, we are interested in exploring an alternative topology
in order to specifically handle cases where the intrinsic dimension of
the data is higher than the dimension of the map.  Most of the time,
the topology of the SOM is one dimensional (linear network) or two
dimensional (regular or hexagonal grid) and this may not correspond to
the intrinsic dimension of the data, especially in the high
dimensional case. This may result in the non-preservation of the
topology \citep{Villmann:1999} with potentially multiple foldings of
the map. The problem is even harder considering the data are unknown
at the time of construction of the network. To overcome this
topological constraint, we propose a variation of the self organizing
map algorithm by considering the random placement of neurons on a
two-dimensional manifold, following a blue noise distribution from
which various topologies can be derived. These topologies possess
random discontinuities that allow for a more flexible
self-organization, especially with high-dimensional data. After
introducing the methods, the model will be illustrated and analyzed
using several classical examples and its properties will be more
finely introduced. Finally, we'll explain how this model can be made
resilient to neural gain or loss by reorganizing the neural sheet
using the centroidal Voronoi tesselation.

A constant issue with self-organizing maps is how can we measure the quality of a map. In SOM's literature, there is neither one measure to rule them all nor a single general recipe on how to measure the quality of the map. Some of the usual measures are the distortion \cite{rynkiewicz:2008}, the $\delta x - \delta y$ representation \citep{Demartines:1992}, and many other specialized measures for rectangular grids or specific types of SOMs \citep{Polani2002}. However, most of those measures cannot be used in this work since we do not use a standard grid for laying over the neural space, instead we use a randomly distributed graph (see supplementary material for standard measures). This and the fact that the neural space is discrete introduce a significant challenge on deciding what will be a good measure for our comparisons \citep{Polani2002} (i.e., to compare the neural spaces of RSOM and regular SOM with the input space). According to \citep{Polani2002}, the quality of the map's organization can be considered equivalent to topology preservation. Therefore, a topological tool such as the persistent homology can help in comparing the input space with the neural one. Topological Data Analysis (TDA) is a relatively new field of applied mathematics and offers a great deal of topological and geometrical tools to analyze point cloud data \citep{Carlsson:2009,HerculanoHouzel:2013}. Such TDA methods have been proposed in \citep{Polani2002}, however TDA wasn't that advanced and popular back then. Therefore, in this work we use the persistent homology and barcodes to analyze our results and compare the neural spaces generated by the SOM algorithms with the input spaces. We provide more details about TDA and persistent homology later in the corresponding section.

To avoid confusion between the original SOM proposed by Teuvo Kohonen and the newly randomized SOM, we'll refer to the original as \textbf{SOM} and the newly randomized one as \textbf{RSOM}.

% \gid{[``In the present review, we wish to point out that the structure introduced in that papers on the discrete space can be interpreted as a complex, a structure known from algebraic topology (Henle 1979). A complex can be seen as a generalization of the notion of a graph. However, no invocation of metric structures is required for its definition, thus the method and the measures derived from it can be regarded as a truly pure topological notions.'']}


% Self-organizing map (SOM) \citep{Kohonen:1982} is a vector quantization method that maps high dimensional data on a low-dimensional grid (usually two-dimensional) through an unsupervised learning process. The low-dimensional discrete map, usually called codebook, consists of code words (vectors) that represent a part of the input space. Two neighboring code words represent similar input samples (prototypes). This is a direct consequence of the underlying topology of the map as well as the learning algorithm. When a new sample is given then the learning algorithm modifies the prototype of  the best matching unit (BMU) as well as the units in its vicinity (neighborhood). SOM have been used in a variety of applications \citep{Kaski:1998,Oja:2003,Polla:2009} and several variations of the original algorithm have been proposed over time \citep{Polla:2009}.

% However, most SOM algorithms assume a fixed neural space (\emph{i.e.}, the space defined by the nodes of the SOM network -- code words)  topology,  which usually is either a rectangular or a hexagonal Cartesian  grid \citep{Astudillo:2014}. This sort of predefined topology of neural space  enforces a rigidity on the neural map and this can lead to a \emph{dimension mismatch} between the input and neural space. This often results in neural representations that are ill-formed and do not cover properly the entire data space. For instance, if the topology of the SOM is one dimensional or  two-dimensional (regular or hexagonal grid) and the intrinsic dimension of the  data is higher than the topology may not be preserved \citep{Villmann:1999}, leading some times to multiple foldings of the map. One of the roots of this problem is the lack of knowledge of the underlying topology of the data space.

% One way to overcome this limitation is to introduce dynamic set of units (neurons) that learn the topology \emph{a posteriori}. Such algorithms are the (i) growing neural gas \citep{Fritzke:1994}, (ii) the incremental grid growing neural network \citep{Blackmore:1995}, and the controlled growth map \citep{Alahakoon:2000}. Nonetheless, the topology in these cases, is built in the data space as opposed to the neural space. This means that the neighborhood property is lost and two nearby neurons on the map may end up with totally different prototypes in the data space. Consequently, these dynamic units do not really solve the problem of preserving the topology and the topological relations between neurons. One solution is to use an alternative topology that allows for more flexibility in the neural space without loosing  performance. We therefore propose in this work a variation of the SOM algorithm by considering the random placement of neurons on a two-dimensional manifold, following a blue noise distribution from which one can derive various different topologies. These topologies possess random but controllable discontinuities that allow for a more flexible self-organization, especially with high-dimensional data.

% We are not the first to explore alternative topologies for training a SOM and the impact of the network topology on self-organization has been studied  before. For instance, \citep{Jiang:2009} consider SOMs whose neighborhood is defined by a regular, small world or random network trained  on the MNIST data set, showing a weak influence of the topology on the performance of the SOM learning algorithm. Furthermore, they optimized the topology of the network using evolutionary algorithms~\citep{Eiben:2003} minimizing the classification error. In this case, their results indicate again a weak correlation between the topology and the performance of the SOM. Another study conducted by \citep{Burguillo:2013} found that the standard Cartesian grid topology was the best over non-conventional topologies (small world, random, and scale-free) for SOMs solving time series prediction problems.

% This paper is organized as follows: first we introduce the necessary terminology and notation. Then we present the model and the learning algorithm as well as the tools to asses the performance of the proposed algorithm. After introducing the model, we conduct several experiments to test the  performance of the algorithm and examine the final topology of the neural space. Finally, we tested the ability of the learning algorithm to cope with situations where reorganization of the neural space is necessary. More precisely,  (i) we perform an ablation study by removing units from the neural space, and (ii) we add extra neurons on the map increasing the capacity of the neural space. In both cases, we show that the topology of the neural space can be preserved.


