
\section{Discussion}

We have introduced a variation of the self-organizing map algorithm by considering the random placement of neurons on a two-dimensional manifold, following a blue noise distribution from which various topologies can be derived. We've shown these topologies possess random (but controllable) discontinuities that allow for a more flexible self-organization, especially with high-dimensional data. This has been demonstrated for low-dimensional cases as well as for high-dimensional case such as the classical MNIST dataset \citep{Lecun:1998}. To analyze the results and characterize properties within the maps, we used tools from the field of topological data analysis and random matrix theory that provide extra information when compared to regular quality measures \citep{Polani2002}. More specifically, we computed the persistence diagrams and barcodes for both the regular and randomized self-organizing maps and the input space and we estimated the eigenvalues distributions of the Gram matrices for the activity of both SOMs. Overall, our results show that the proposed algorithm performs equally well as the original SOM and develop well-formed topographic maps. In some cases, RSOM preserves actually better the topological properties of the input space when compared to the original SOM but it is difficult to assert that this is a general property since a theoretical approach would be a hard problem. Another important aspect we highlighted is that RSOM can cope with the addition or the removal of units during learning and preserve, to a large extent, the underlying self-organization. This reorganization capacity allows to have an adaptive architecture where neurons can be added or removed following an arbitrary quality criterion.

This article comes last in a series of three articles where we investigated
the conditions for a more biologically plausible self-organization process. In the first article \citep{Rougier:2017}, we introduced the dynamic SOM (DSOM) and showed how the time-dependent learning rate and neighborhood function variance of regular SOM can be replaced by a time-independent learning process. DSOM is capable of continuous on-line learning and can adapt anytime to a dynamic dataset. In the second article \citep{Detorakis2012, Detorakis2014}, we introduced the dynamic neural field SOM (DNF-SOM) where the winner-take-all competitive stage has been replaced by a regular neural field that aimed at simulating the neural activity of the somatosensory cortex (area 3b). The whole SOM procedure is thus replaced by an actual distributed process without the need of any supervisor to select the BMU. The selection of the BMU as well as the neighborhood function emerge naturally
%It naturally emerges as a Gaussian bump
due to the lateral competition between neurons that ultimately drives the self-organization. The present work is the last part of this sequel and provides the basis for developing biologically plausible self-organizing maps. Taken together, DSOM, DNF-SOM and RSOM provides a biological ground for self-organization where decreasing learning rate, winner-take-all and regular grid are not necessary. Instead, our main hypotheses are the blue noise distribution and the nearest-neighbour connectivity pattern. For the blue noise distribution and given the physical nature of neurons \citep{BlazquezLlorca2014,Lanaro:2020}, we think it makes sense to consider neurons to be at a minimal distance from each others and randomly distributed and to have a nearest-neighbour connectivity as it is known to occur in the cortex \citep{vanPelt2013}. The case of reorganization, where neurons physically migrate (Lloyd relaxation), is probably the most dubious hypothesis but seems to be partially supported by experimental results
\citep{Kaneko2017}. It is also worth to mention that reorganization takes place naturally in the mammal brain. More precisely, neurogenesis happens in the subgranular zone of the dentate gyrus of the hippocampus and in the subventricular zone of the lateral ventricle \citep{Alvarez:2004}. On the other hand, when neural tissue in the cerebral cortex \citep{Merzenich:1984,Taub:2014} or the spinal cord \citep{Bareyre:2004,Liu:1958} are damaged, neurons reorganize their receptive fields and undamaged nerves sprout new connections and restore function (partially or fully). During such event, it has been shown that neurons can physically move.

% \gid{Maybe we could fit these sentences somewhere:
Finally, the analysis we performed (TDA, eigenvalues distributions, distortion, and entropy indicates that both SOM and RSOM perform equally well. For the majority of the measures we used to assess the performance of both algorithms, we observed very similar results. Only in the case of TDA, we identified some differences in the topological features the two algorithms can capture. More precisely, both algorithms generate maps that capture most of the topological features of the input space. RSOM tends to capture slightly better high-dimensional topological features, especially for input spaces with holes (see the experiment on the $2D$ uniform distribution in Section~\ref{sec:2d-holes}). Therefore, we can conclude that the RSOM matches the performance of the SOM.

%It is worth mentioning that RSOM provides a more biologically plausible mechanism that has the same performance as the regular SOM and captures equally well the topological features of the input space. }

%As future work, we envision a unified framework that would account for the development of biologically plausible topographic maps of the primary sensory cortices  and would be able to cope with reorganization in all three cases: (i) neural units loss, (ii) afferent information stream loss, and (iii) neurogenesis. 

%\gid{Notes
%\begin{itemize}
    %\item Blue noise might be ideal for biologically plausible algorithms since       it carries high frequencies. Usually, physical systems compress data based on an energy functional and it is common to keep the highest frequencies (see Wavelets and Multilevel Wavelet Decomposition).
%    \item Neurons are never arranged on a perfectly organized Cartesian grid. It's more likely that they will be organized in a more \emph{unordered} fashion. 
%    \item Not to compete against Kohonen (Kohonen is simple and usefull) but  to propose a new self-organizing map algorithm that used more biologically plausible components. 
%\end{itemize}
%}

% The DNF-SOM is similar to DSOM in the sense that it provides continual on-line learning. Moreover, DNF-SOM can cope with cortical lesions and sensory deprivations meaning that it can reorganize its topographic maps.


%is the case of reorganization of the map. The reorganization of the receptive fields of neurons is imperative in cases where some of the neural units have been damaged (ablation of neurons) or new units emerge (neurogenesis). In both cases our algorithm learns successfully to reorganize the map to compensate  either for a loss of neurons or for the genesis of new units. 


 %We used a two-dimensional map and we tried to learn the representations of: (i) one-dimensional uniform distributions,  (ii) two-dimensional uniform distributions (mostly with the presence of geometrical holes), (iii) three-dimensional uniform distributions and finally the MNIST hand written digits data set.
% \npr{Need to cite  and explain why we used the topology analysis}

% Here, we have shown that self-organizing maps can emerge from neurons (units) that are arranged not on a Euclidean rectangular grid as in the seminal Kohonen's map~\cite{Kohonen:1982} but on a randomly determined grid. The grid follows a blue noise distribution more precisely we use  the method of Poisson discs to place the neurons on random spots and connect them together.
